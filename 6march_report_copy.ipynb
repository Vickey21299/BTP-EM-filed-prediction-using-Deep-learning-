{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved E_field_data_filtered_1.npz with 6708 records (Train angles: {90, 60, 30})\n",
      "Saved E_field_data_filtered_test_1.npz with 2236 records (Test angle: {0})\n",
      "Saved E_field_data_filtered_2.npz with 6708 records (Train angles: {0, 90, 60})\n",
      "Saved E_field_data_filtered_test_2.npz with 2236 records (Test angle: {30})\n",
      "Saved E_field_data_filtered_3.npz with 6708 records (Train angles: {0, 90, 30})\n",
      "Saved E_field_data_filtered_test_3.npz with 2236 records (Test angle: {60})\n",
      "Saved E_field_data_filtered_4.npz with 6708 records (Train angles: {0, 60, 30})\n",
      "Saved E_field_data_filtered_test_4.npz with 2236 records (Test angle: {90})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the original data\n",
    "loaded_data = np.load('E_field_data_combined_extended.npz', allow_pickle=True)\n",
    "E_data_combined = loaded_data['data']\n",
    "\n",
    "# Define the full set of angles\n",
    "all_angles = {0, 30, 60, 90}\n",
    "\n",
    "# Define filter sets for training (leave-one-out approach)\n",
    "filter_sets = [\n",
    "    {30, 60, 90},  # Training: {30, 60, 90} → Testing: {0}\n",
    "    {0, 60, 90},   # Training: {0, 60, 90} → Testing: {30}\n",
    "    {0, 30, 90},   # Training: {0, 30, 90} → Testing: {60}\n",
    "    {0, 30, 60}    # Training: {0, 30, 60} → Testing: {90}\n",
    "]\n",
    "\n",
    "# Iterate through each filter set\n",
    "for idx, train_angles in enumerate(filter_sets):\n",
    "    test_angles = all_angles - train_angles  # Find the test angle (only one value)\n",
    "\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    # Filter the dataset\n",
    "    for i in E_data_combined:\n",
    "        j, k, l = i[:3]  # Extract first three values (j, k, l)\n",
    "        if j in train_angles:\n",
    "            train_data.append(i)\n",
    "        elif j in test_angles:\n",
    "            test_data.append(i)\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    train_data = np.array(train_data)\n",
    "    test_data = np.array(test_data)\n",
    "\n",
    "    # Save train and test datasets\n",
    "    filename_train = f\"E_field_data_filtered_{idx+1}.npz\"\n",
    "    filename_test = f\"E_field_data_filtered_test_{idx+1}.npz\"\n",
    "\n",
    "    np.savez(filename_train, data=train_data)\n",
    "    np.savez(filename_test, data=test_data)\n",
    "\n",
    "    print(f\"Saved {filename_train} with {len(train_data)} records (Train angles: {train_angles})\")\n",
    "    print(f\"Saved {filename_test} with {len(test_data)} records (Test angle: {test_angles})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "  X_train Shape: (6708, 4), y_train Shape: (6708, 97, 177)\n",
      "  X_test Shape: (2236, 4), y_test Shape: (2236, 97, 177)\n",
      "----------------------------------------\n",
      "Dataset 2:\n",
      "  X_train Shape: (6708, 4), y_train Shape: (6708, 97, 177)\n",
      "  X_test Shape: (2236, 4), y_test Shape: (2236, 97, 177)\n",
      "----------------------------------------\n",
      "Dataset 3:\n",
      "  X_train Shape: (6708, 4), y_train Shape: (6708, 97, 177)\n",
      "  X_test Shape: (2236, 4), y_test Shape: (2236, 97, 177)\n",
      "----------------------------------------\n",
      "Dataset 4:\n",
      "  X_train Shape: (6708, 4), y_train Shape: (6708, 97, 177)\n",
      "  X_test Shape: (2236, 4), y_test Shape: (2236, 97, 177)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of datasets\n",
    "num_datasets = 4  \n",
    "\n",
    "# Iterate through each dataset\n",
    "for idx in range(1, num_datasets + 1):\n",
    "    train_file = f\"E_field_data_filtered_{idx}.npz\"\n",
    "    test_file = f\"E_field_data_filtered_test_{idx}.npz\"\n",
    "\n",
    "    # Load training and testing data\n",
    "    train_data = np.load(train_file, allow_pickle=True)['data']\n",
    "    test_data = np.load(test_file, allow_pickle=True)['data']\n",
    "\n",
    "    # Extract features (conditions) and target (matrices)\n",
    "    X_train = train_data[:, :-1].astype(np.float32)\n",
    "    y_train = np.array([np.array(matrix, dtype=np.float32) for matrix in train_data[:, -1]])\n",
    "    y_train = y_train[:, 2:99, 2:179]  # Extract the required submatrix\n",
    "\n",
    "    X_test = test_data[:, :-1].astype(np.float32)\n",
    "    y_test = np.array([np.array(matrix, dtype=np.float32) for matrix in test_data[:, -1]])\n",
    "    y_test = y_test[:, 2:99, 2:179]\n",
    "\n",
    "    # Print shapes for verification\n",
    "    print(f\"Dataset {idx}:\")\n",
    "    print(f\"  X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}\")\n",
    "    print(f\"  X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is Update version with sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ranges : [39 60  0 76 65 25 52 10 57 22 61 63 17  1 45 34 44 59 46 66 28 68 73  3\n",
      " 29 53 30 56 55 47 75 18 77 20 51 38  5 26  7 41  4 49 33 40 74  9 12 13\n",
      " 50 11 70 14 42 67 71 23 27 21 48 36  6  2 16  8 62 24 43 72 15 19]\n",
      "val_ranges : [64 35 58 31 37 69 32 54]\n",
      "X_train Shape: (6020, 4), y_train Shape: (6020, 97, 177)\n",
      "X_val Shape: (688, 4), y_val Shape: (688, 97, 177)\n",
      "X_test Shape: (2236, 4), y_test Shape: (2236, 97, 177)\n",
      "y_train min: 0.0, max: 1.0\n",
      "y_val min: 0.01858055405318737, max: 0.873414933681488\n",
      "y_test min: 0.16905874013900757, max: 0.9943581223487854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_data(train_file, test_file, samples_per_range=86, train_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the dataset, including train-validation split and normalization.\n",
    "\n",
    "    Parameters:\n",
    "    - train_file (str): Path to the training data file (.npz).\n",
    "    - test_file (str): Path to the test data file (.npz).\n",
    "    - samples_per_range (int): Number of samples per range.\n",
    "    - train_ratio (float): Proportion of data used for training (default 90%).\n",
    "\n",
    "    Returns:\n",
    "    - X_train, y_train (normalized)\n",
    "    - X_val, y_val (normalized)\n",
    "    - X_test, y_test (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the training and test data\n",
    "    train_data = np.load(train_file, allow_pickle=True)['data']\n",
    "    test_data = np.load(test_file, allow_pickle=True)['data']\n",
    "    \n",
    "    total_samples = train_data.shape[0]\n",
    "    num_ranges = total_samples // samples_per_range\n",
    "\n",
    "    # Shuffle and split training-validation data\n",
    "    all_ranges = np.arange(num_ranges)\n",
    "    np.random.shuffle(all_ranges)\n",
    "    \n",
    "    train_ranges = all_ranges[:int(train_ratio * num_ranges)]\n",
    "    val_ranges = all_ranges[int(train_ratio * num_ranges):]  # Remaining for validation\n",
    "    print(\"train_ranges :\" , train_ranges)\n",
    "    print(\"val_ranges :\", val_ranges)\n",
    "\n",
    "    def get_indices_from_ranges(ranges, samples_per_range):\n",
    "        indices = []\n",
    "        for r in ranges:\n",
    "            start = r * samples_per_range\n",
    "            end = start + samples_per_range\n",
    "            indices.extend(range(start, end))\n",
    "        return indices\n",
    "\n",
    "    # Get indices for train and validation sets\n",
    "    train_indices = get_indices_from_ranges(train_ranges, samples_per_range)\n",
    "    val_indices = get_indices_from_ranges(val_ranges, samples_per_range)\n",
    "    \n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "\n",
    "    # Extract features (X) and target (y)\n",
    "    X_full = train_data[:, :-1].astype(np.float32)\n",
    "    y_full = np.array([np.array(matrix, dtype=np.float32) for matrix in train_data[:, -1]])\n",
    "    y_full = y_full[:, 2:99, 2:179]  # Extract submatrix\n",
    "\n",
    "    # Train-Validation Split\n",
    "    X_train, X_val = X_full[train_indices], X_full[val_indices]\n",
    "    y_train, y_val = y_full[train_indices], y_full[val_indices]\n",
    "\n",
    "    # Load test data\n",
    "    X_test = test_data[:, :-1].astype(np.float32)\n",
    "    y_test = np.array([np.array(matrix, dtype=np.float32) for matrix in test_data[:, -1]])\n",
    "    y_test = y_test[:, 2:99, 2:179]\n",
    "\n",
    "    # **Normalization Using Min-Max Scaling**\n",
    "    y_min = y_train.min()\n",
    "    y_max = y_train.max()\n",
    "\n",
    "    y_train = (y_train - y_min) / (y_max - y_min)\n",
    "    y_val = (y_val - y_min) / (y_max - y_min)\n",
    "    y_test = (y_test - y_min) / (y_max - y_min)  # Normalize using y_train's min-max\n",
    "\n",
    "    # Print shapes and min-max values for verification\n",
    "    print(f\"X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}\")\n",
    "    print(f\"X_val Shape: {X_val.shape}, y_val Shape: {y_val.shape}\")\n",
    "    print(f\"X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}\")\n",
    "    print(f\"y_train min: {y_train.min()}, max: {y_train.max()}\")\n",
    "    print(f\"y_val min: {y_val.min()}, max: {y_val.max()}\")\n",
    "    print(f\"y_test min: {y_test.min()}, max: {y_test.max()}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Example Usage:\n",
    "train_file = \"E_field_data_filtered_1.npz\"\n",
    "test_file = \"E_field_data_filtered_test_1.npz\"\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_and_preprocess_data(train_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now , we are update the code and use the the check point and droupout and earily stopping  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the best_model.h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:43:06.476866: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-19 04:43:06.549543: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Model Architecture\n",
    "def model_architecture():\n",
    "    alpha = 0.0  # Leaky ReLU parameter\n",
    "\n",
    "    def dense_block(input, num_units):\n",
    "        x = Dense(num_units)(input)\n",
    "        x = LeakyReLU(alpha=alpha)(x)\n",
    "        return x\n",
    "\n",
    "    inputs = Input(shape=(4,), name='data')\n",
    "\n",
    "    x = dense_block(inputs, 8)\n",
    "    x = dense_block(x, 32)\n",
    "    x = dense_block(x, 128)\n",
    "    x = dense_block(x, 512)\n",
    "    x = dense_block(x, 2048)\n",
    "    x = dense_block(x, 8192)\n",
    "    x = Dense(97 * 177, activation='sigmoid')(x)\n",
    "\n",
    "    outputs = Reshape((97, 177, 1))(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"fcn_conv\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train model with batch size\n",
    "def train_with_batch_size(batch_size, X_train, X_val, y_train, y_val, file_name):\n",
    "    print(f\"\\nTraining on {file_name} with batch size: {batch_size}\")\n",
    "\n",
    "    # Build and compile model\n",
    "    model = model_architecture()\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
    "\n",
    "    # Reshape labels\n",
    "    y_train_reshaped = np.expand_dims(y_train, axis=-1)\n",
    "    y_val_reshaped = np.expand_dims(y_val, axis=-1)\n",
    "\n",
    "    # Remove .npz from file_name\n",
    "    file_base = os.path.splitext(file_name)[0]  # Removes the .npz extension\n",
    "\n",
    "    # Define checkpoint filename\n",
    "    checkpoint_filename = f'best_model_{file_base}_batch_{batch_size}.keras'\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filename, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train_reshaped,\n",
    "        validation_data=(X_val, y_val_reshaped),\n",
    "        epochs=1000, batch_size=batch_size, verbose=1,\n",
    "        callbacks=[checkpoint, early_stopping]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "# Example usage:\n",
    "# file_name = \"E_field_data_filtered_1.npz\"  # Example file\n",
    "# history = train_with_batch_size(32, X_train, X_val, y_train, y_val, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: E_field_data_filtered_1.npz with Test File: E_field_data_filtered_test_1.npz\n",
      "train_ranges : [ 9 46 66 57 12 72 60 43  5 33  8 24 19 28 21 29  1 31 53 63 13 34 45 11\n",
      "  2 30 56  4 20 74 17 58  3 70 65 64 49 68 55 59 75 51 22 26 41 47  7 48\n",
      " 37 67 40 32 76 54 62  6 44 15 23 36 61 25 77 69 39 16  0 10 42 52]\n",
      "val_ranges : [18 38 27 14 71 35 50 73]\n",
      "X_train Shape: (6020, 4), y_train Shape: (6020, 97, 177)\n",
      "X_val Shape: (688, 4), y_val Shape: (688, 97, 177)\n",
      "X_test Shape: (2236, 4), y_test Shape: (2236, 97, 177)\n",
      "y_train min: 0.0, max: 1.0\n",
      "y_val min: 0.007142702117562294, max: 0.9295064210891724\n",
      "y_test min: 0.16905874013900757, max: 0.9943581223487854\n",
      "X_train shape: (6020, 4), y_train shape: (6020, 97, 177)\n",
      "\n",
      "Training on E_field_data_filtered_1.npz with batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:43:09.643858: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-19 04:43:09.792773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 44197 MB memory:  -> device: 0, name: NVIDIA L40S, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:43:11.464342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-03-19 04:43:11.521905: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7eff927d0f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-03-19 04:43:11.521940: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA L40S, Compute Capability 8.9\n",
      "2025-03-19 04:43:11.532143: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-03-19 04:43:11.615481: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2025-03-19 04:43:11.615508: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:237] Used ptxas at ptxas\n",
      "2025-03-19 04:43:11.615629: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:281] Couldn't read CUDA driver version.\n",
      "2025-03-19 04:43:11.616384: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376/377 [============================>.] - ETA: 0s - loss: 0.0395\n",
      "Epoch 1: val_loss improved from inf to 0.03050, saving model to best_model_E_field_data_filtered_1_batch_16.keras\n",
      "377/377 [==============================] - 15s 36ms/step - loss: 0.0395 - val_loss: 0.0305\n",
      "Epoch 2/1000\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.0343\n",
      "Epoch 2: val_loss did not improve from 0.03050\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.0343 - val_loss: 0.0346\n",
      "Epoch 3/1000\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.0326\n",
      "Epoch 3: val_loss improved from 0.03050 to 0.02349, saving model to best_model_E_field_data_filtered_1_batch_16.keras\n",
      "377/377 [==============================] - 15s 40ms/step - loss: 0.0326 - val_loss: 0.0235\n",
      "Epoch 4/1000\n",
      "376/377 [============================>.] - ETA: 0s - loss: 0.0291\n",
      "Epoch 4: val_loss did not improve from 0.02349\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.0291 - val_loss: 0.0244\n",
      "Epoch 5/1000\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.0257\n",
      "Epoch 5: val_loss improved from 0.02349 to 0.01916, saving model to best_model_E_field_data_filtered_1_batch_16.keras\n",
      "377/377 [==============================] - 16s 41ms/step - loss: 0.0257 - val_loss: 0.0192\n",
      "Epoch 6/1000\n",
      "375/377 [============================>.] - ETA: 0s - loss: 0.0230\n",
      "Epoch 6: val_loss did not improve from 0.01916\n",
      "377/377 [==============================] - 14s 36ms/step - loss: 0.0230 - val_loss: 0.0222\n",
      "Epoch 7/1000\n",
      "374/377 [============================>.] - ETA: 0s - loss: 0.0199\n",
      "Epoch 7: val_loss did not improve from 0.01916\n",
      "377/377 [==============================] - 13s 35ms/step - loss: 0.0198 - val_loss: 0.0207\n",
      "Epoch 8/1000\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.0192\n",
      "Epoch 8: val_loss improved from 0.01916 to 0.01574, saving model to best_model_E_field_data_filtered_1_batch_16.keras\n",
      "377/377 [==============================] - 16s 43ms/step - loss: 0.0192 - val_loss: 0.0157\n",
      "Epoch 9/1000\n",
      "376/377 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 9: val_loss did not improve from 0.01574\n",
      "377/377 [==============================] - 15s 39ms/step - loss: 0.0157 - val_loss: 0.0177\n",
      "Epoch 10/1000\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.0158\n",
      "Epoch 10: val_loss improved from 0.01574 to 0.01440, saving model to best_model_E_field_data_filtered_1_batch_16.keras\n",
      "377/377 [==============================] - 17s 44ms/step - loss: 0.0158 - val_loss: 0.0144\n",
      "Epoch 11/1000\n",
      "375/377 [============================>.] - ETA: 0s - loss: 0.0148\n",
      "Epoch 11: val_loss did not improve from 0.01440\n",
      "377/377 [==============================] - 14s 38ms/step - loss: 0.0148 - val_loss: 0.0200\n",
      "Epoch 12/1000\n",
      "376/377 [============================>.] - ETA: 0s - loss: 0.0139\n",
      "Epoch 12: val_loss did not improve from 0.01440\n",
      "377/377 [==============================] - 14s 36ms/step - loss: 0.0139 - val_loss: 0.0239\n",
      "Epoch 13/1000\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.0136\n",
      "Epoch 13: val_loss did not improve from 0.01440\n",
      "377/377 [==============================] - 13s 34ms/step - loss: 0.0136 - val_loss: 0.0171\n",
      "Epoch 14/1000\n",
      "376/377 [============================>.] - ETA: 0s - loss: 0.0119\n",
      "Epoch 14: val_loss improved from 0.01440 to 0.01240, saving model to best_model_E_field_data_filtered_1_batch_16.keras\n",
      "377/377 [==============================] - 16s 43ms/step - loss: 0.0118 - val_loss: 0.0124\n",
      "Epoch 15/1000\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.0121\n",
      "Epoch 15: val_loss did not improve from 0.01240\n",
      "377/377 [==============================] - 13s 35ms/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 16/1000\n",
      "374/377 [============================>.] - ETA: 0s - loss: 0.0118\n",
      "Epoch 16: val_loss did not improve from 0.01240\n",
      "377/377 [==============================] - 13s 35ms/step - loss: 0.0118 - val_loss: 0.0140\n",
      "Epoch 17/1000\n",
      "377/377 [==============================] - ETA: 0s - loss: 0.0113\n",
      "Epoch 17: val_loss improved from 0.01240 to 0.01121, saving model to best_model_E_field_data_filtered_1_batch_16.keras\n",
      "377/377 [==============================] - 16s 44ms/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 18/1000\n",
      "375/377 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 18: val_loss did not improve from 0.01121\n",
      "377/377 [==============================] - 14s 37ms/step - loss: 0.0109 - val_loss: 0.0162\n",
      "Epoch 19/1000\n",
      "376/377 [============================>.] - ETA: 0s - loss: 0.0100\n",
      "Epoch 19: val_loss improved from 0.01121 to 0.00803, saving model to best_model_E_field_data_filtered_1_batch_16.keras\n",
      "377/377 [==============================] - 18s 47ms/step - loss: 0.0100 - val_loss: 0.0080\n",
      "Epoch 20/1000\n",
      "376/377 [============================>.] - ETA: 0s - loss: 0.0103\n",
      "Epoch 20: val_loss did not improve from 0.00803\n",
      "377/377 [==============================] - 12s 32ms/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 21/1000\n",
      "376/377 [============================>.] - ETA: 0s - loss: 0.0105\n",
      "Epoch 21: val_loss did not improve from 0.00803\n",
      "377/377 [==============================] - 12s 32ms/step - loss: 0.0105 - val_loss: 0.0099\n",
      "Epoch 22/1000\n",
      "367/377 [============================>.] - ETA: 0s - loss: 0.0097"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m train_file_base \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(train_file)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Removes the .npz extension\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Train and get history\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     history, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_file_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mtrain_with_batch_size\u001b[0;34m(batch_size, X_train, X_val, y_train, y_val, file_name)\u001b[0m\n\u001b[1;32m     55\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_reshaped\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/engine/training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1655\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1656\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    349\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/callbacks.py:394\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    393\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 394\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/callbacks.py:1094\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/utils/tf_utils.py:665\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/keras/utils/tf_utils.py:658\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 658\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1155\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/miniconda3/envs/Vickeyconda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1121\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1120\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# List of training and test files\n",
    "train_files = [\n",
    "    \"E_field_data_filtered_1.npz\",\n",
    "    \"E_field_data_filtered_2.npz\",\n",
    "    \"E_field_data_filtered_3.npz\",\n",
    "    \"E_field_data_filtered_4.npz\"\n",
    "]\n",
    "\n",
    "test_files = [\n",
    "    \"E_field_data_filtered_test_1.npz\",\n",
    "    \"E_field_data_filtered_test_2.npz\",\n",
    "    \"E_field_data_filtered_test_3.npz\",\n",
    "    \"E_field_data_filtered_test_4.npz\"\n",
    "]\n",
    "\n",
    "# Different batch sizes to experiment with\n",
    "batch_sizes = [16]\n",
    "\n",
    "# Train on each file\n",
    "for train_file, test_file in zip(train_files, test_files):\n",
    "    print(f\"\\nProcessing: {train_file} with Test File: {test_file}\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_preprocess_data(train_file, test_file)\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # Remove .npz extension from filenames\n",
    "    train_file_base = os.path.splitext(train_file)[0]  # Removes the .npz extension\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        # Train and get history\n",
    "        history, model = train_with_batch_size(batch_size, X_train, X_val, y_train, y_val, train_file)\n",
    "\n",
    "        # Save the trained model\n",
    "        model_filename = f\"best_model_{train_file_base}_batch_{batch_size}.keras\"\n",
    "        model.save(model_filename)\n",
    "        print(f\"Model saved as: {model_filename}\")\n",
    "\n",
    "        # Plot the loss curves\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Loss Curve - {train_file_base} (Batch {batch_size})')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        # Evaluate on test data\n",
    "        y_test_reshaped = np.expand_dims(y_test, axis=-1)  # Reshape for evaluation\n",
    "        test_loss = model.evaluate(X_test, y_test_reshaped, verbose=1)\n",
    "        print(f\"Test Loss on {test_file} (Batch {batch_size}): {test_loss:.4f}\")\n",
    "\n",
    "        # Load the trained model\n",
    "        loaded_model = load_model(model_filename)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_best_model = loaded_model.predict(X_test)\n",
    "\n",
    "        # Ensure prediction shape matches y_test\n",
    "        y_pred_best_model_squeezed = np.squeeze(y_pred_best_model)  # Remove extra dimension if necessary\n",
    "\n",
    "        # Calculate MAE for each sample and its variance\n",
    "        err = np.array([np.mean(np.abs(y_test[i, ...] - y_pred_best_model_squeezed[i, ...])) for i in range(y_test.shape[0])])\n",
    "        test_mae = np.mean(err)\n",
    "        test_variance = np.var(err)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Test MAE on {test_file}: {test_mae:.4f}\")\n",
    "        print(f\"Variance of MAE across samples: {test_variance:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vickeyconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
